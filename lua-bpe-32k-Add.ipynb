{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d1582-60d4-479f-841c-be2ed0a291fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c7b348-8ec5-41ba-b4bb-df66745484aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting sentencepiece\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/04/88/14f2f4a2b922d8b39be45bf63d79e6cd3a9b2f248b2fcb98a69b12af12f5/sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73dc9d21-3c10-4401-b7c9-2b30b71a3c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3720292483.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m--input=all.txt \\\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#bash\n",
    "spm_train \\\n",
    "  --input=all.txt \\\n",
    "  --model_prefix=lua-bpe-32k \\\n",
    "  --vocab_size=32768 \\\n",
    "  --character_coverage=0.9995 \\\n",
    "  --model_type=bpe \\\n",
    "  --byte_fallback=true \\\n",
    "  --split_digits=true \\\n",
    "  --split_by_unicode_script=true \\\n",
    "  --allow_whitespace_only_pieces=true \\\n",
    "  --remove_extra_whitespaces=false \\\n",
    "  --normalization_rule_name=nfkc \\\n",
    "  --unk_piece='<|TOKEN:unk|>' \\\n",
    "  --bos_piece='<|endoftext|>' \\\n",
    "  --eos_piece='<|im_end|>' \\\n",
    "  --pad_piece='<|TOKEN:pad|>' \\\n",
    "  --unk_id=0 \\\n",
    "  --bos_id=1 \\\n",
    "  --eos_id=2 \\\n",
    "  --pad_id=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1310e-f7f3-49a8-b38d-e467e4d1c16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='./all.txt',\n",
    "    model_prefix='lua-bpe-32k',\n",
    "    vocab_size=32*1024,\n",
    "    character_coverage=0.9995,\n",
    "    model_type='bpe',\n",
    "    byte_fallback=True,\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    remove_extra_whitespaces=False,\n",
    "    normalization_rule_name=\"nfkc\",\n",
    "    unk_piece=\"<|TOKEN:unk|>\",\n",
    "    bos_piece=\"<|endoftext|>\",\n",
    "    eos_piece=\"<|im_end|>\",\n",
    "    pad_piece=\"<|TOKEN:pad|>\",\n",
    "    unk_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    pad_id=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cd906b-6eb8-43f7-b65b-fcb2da38464f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CODE-001.jsonl.gz ...\n",
      "Processing CODE-002.jsonl.gz ...\n",
      "Processing CODE-003.jsonl.gz ...\n",
      "Processing CODE-004.jsonl.gz ...\n",
      "Processing CODE-005.jsonl.gz ...\n",
      "Done. 全部 lua 内容已写入 /root/autodl-tmp/all.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "merge_lua.py\n",
    "把 A 目录下所有 xxx.jsonl.gz 文件里的 \"lua\" 字段提取出来\n",
    "写入到 all.txt（一行一条 lua 脚本内容）。\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "SRC_DIR = Path(\"./\")           # 原始数据目录\n",
    "PATTERN = \"*.jsonl.gz\"        # 匹配规则\n",
    "DST_FILE = Path(\"all.txt\")    # 输出文件\n",
    "\n",
    "def extract_lua_fields():\n",
    "    # 如果目标文件已存在，可改为 'a' 追加；这里用 'w' 覆盖。\n",
    "    with DST_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        # 按文件名排序，保持确定性顺序（可选）\n",
    "        for gz_path in sorted(SRC_DIR.glob(PATTERN)):\n",
    "            print(f\"Processing {gz_path} ...\")\n",
    "            with gzip.open(gz_path, \"rt\", encoding=\"utf-8\") as fin:\n",
    "                for line_no, line in enumerate(fin, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:          # 跳过空行\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[WARN] {gz_path}:{line_no} JSON 解析失败：{e}\")\n",
    "                        continue\n",
    "                    lua_code = obj.get(\"lua\")\n",
    "                    if lua_code is None:\n",
    "                        print(f\"[WARN] {gz_path}:{line_no} 没有 'lua' 字段\")\n",
    "                        continue\n",
    "                    # 每段 lua 内容占一行；如内容本身含换行需额外处理\n",
    "                    fout.write(lua_code.rstrip(\"\\n\") + \"\\n\")\n",
    "    print(f\"Done. 全部 lua 内容已写入 {DST_FILE.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_lua_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8182749b-e951-442a-b693-055dab47a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '这', '老', '者', '<0xE5>', '<0xA7>', '<0x93>', '左', ',', '名', '<0xE5>', '<0x8F>', '<0xAB>', '子', '<0xE7>', '<0xA9>', '<0x86>', ',', '是', '“', '无', '量', '剑', '”', '东', '<0xE5>', '<0xAE>', '<0x97>', '的', '<0xE6>', '<0x8E>', '<0x8C>', '门', '。', '那', '道', '<0xE5>', '<0xA7>', '<0x91>', '<0xE5>', '<0xA7>', '<0x93>', '<0xE8>', '<0xBE>', '<0x9B>', ',', '道', '号', '双', '清', ',', '是', '“', '无', '量', '剑', '”', '西', '<0xE5>', '<0xAE>', '<0x97>', '<0xE6>', '<0x8E>', '<0x8C>', '门', '。']\n",
      "[30888, 31391, 32061, 31109, 233, 171, 151, 32014, 30900, 31177, 233, 147, 175, 31161, 235, 173, 138, 30900, 31117, 31331, 31353, 31245, 31834, 31299, 32555, 233, 178, 155, 31025, 234, 146, 144, 31976, 31050, 32281, 31688, 233, 171, 149, 233, 171, 151, 236, 194, 159, 30900, 31688, 31694, 32299, 31970, 30900, 31117, 31331, 31353, 31245, 31834, 31299, 32092, 233, 178, 155, 234, 146, 144, 31976, 31050]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "# https://zhuanlan.zhihu.com/p/669328671\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"./lua-bpe-32k/lua-bpe-32k.model\")\n",
    "\n",
    "print(sp.encode_as_pieces(\"这老者姓左，名叫子穆，是“无量剑”东宗的掌门。那道姑姓辛，道号双清，是“无量剑”西宗掌门。\"))\n",
    "print(sp.encode_as_ids(\"这老者姓左，名叫子穆，是“无量剑”东宗的掌门。那道姑姓辛，道号双清，是“无量剑”西宗掌门。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20378b93-124b-4b10-887a-f4d7a53dfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall protobuf\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e9a84-9a99-4748-97aa-06de9190289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "\n",
    "llama_tokenizer_dir = 'llama-2-7b-bin'\n",
    "chinese_sp_model_file = './lua-bpe-32k/lua-bpe-32k.model'\n",
    "\n",
    "# 分词器加载\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "\n",
    "# 解析\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# 词表长度\n",
    "print(len(llama_tokenizer),len(chinese_sp_model))\n",
    "\n",
    "# 添加新token到llama词表\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "output_sp_dir = '../merged_tokenizer_sp'\n",
    "output_hf_dir = '../merged_tokenizer_hf'\n",
    "\n",
    "vocab_content = ''\n",
    "for p in llama_spm.pieces:\n",
    "    vocab_content += f\"{p.piece} {p.score}\\n\"\n",
    "# 保存词表\n",
    "with open(output_sp_dir+'/llama.vocab', \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(vocab_content)\n",
    "# 保存spm模型\n",
    "with open(output_sp_dir+'/llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "\n",
    "# 保存llama新tokenizer\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/llama.model')\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583ff89-f3c2-4f49-963a-882102e8876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lua-bpe-32k.zip lua-bpe-32k.model lua-bpe-32k.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb477cb6-83f9-4743-8d3b-e00dd9022812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.12/site-packages (6.32.0)\n",
      "Requirement already satisfied: tokenizers in /root/miniconda3/lib/python3.12/site-packages (0.21.4)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.12/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (2025.7.33)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (4.66.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[sentencepiece]\" \"protobuf\" \"tokenizers\" \"transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff4f08d-24aa-4eff-90ee-25f0747fb39d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformers <command> [<args>] convert [-h] --model_type MODEL_TYPE\n",
      "                                               --tf_checkpoint TF_CHECKPOINT\n",
      "                                               --pytorch_dump_output\n",
      "                                               PYTORCH_DUMP_OUTPUT\n",
      "                                               [--config CONFIG]\n",
      "                                               [--finetuning_task_name FINETUNING_TASK_NAME]\n",
      "transformers <command> [<args>] convert: error: the following arguments are required: --tf_checkpoint, --pytorch_dump_output\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli convert --model_type gpt2 --tokenizer_name_or_path ./lua-bpe-32k.model --output_dir ./hf-tokenizer --tokenizer_class PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8639d1d9-967a-40ab-8233-91cdf901f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# source /etc/network_turbo\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70288bbe-851c-4db5-8511-c43ddf1d0c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-23 18:38:34--  https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py\n",
      "Connecting to 10.37.1.23:12798... connected.\n",
      "WARNING: cannot verify raw.githubusercontent.com's certificate, issued by ‘emailAddress=autodl@gmail.com,CN=autodl,OU=autodl,O=autodl,L=beijing,ST=beijing,C=CN’:\n",
      "  Self-signed certificate encountered.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 6257 (6.1K) [text/plain]\n",
      "Saving to: ‘sentencepiece_model_pb2.py’\n",
      "\n",
      "sentencepiece_model 100%[===================>]   6.11K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-08-23 18:38:35 (190 KB/s) - ‘sentencepiece_model_pb2.py’ saved [6257/6257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dd77a-892f-463a-84f9-6d03e8e8357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall protobuf\n",
    "!pip install \"protobuf<3.20\"\n",
    "!pip install tokenizers sentencepiece transformers\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa21cce-1ad9-4bb3-9b5f-e95e1306ffdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lua-bpe-32k-hf-V2/tokenizer_config.json',\n",
       " 'lua-bpe-32k-hf-V2/special_tokens_map.json',\n",
       " 'lua-bpe-32k-hf-V2/tokenizer.model',\n",
       " 'lua-bpe-32k-hf-V2/added_tokens.json',\n",
       " 'lua-bpe-32k-hf-V2/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "# 选择适合的 Tokenizer 类，通常 SentencePiece 适合 T5Tokenizer / LlamaTokenizer\n",
    "from transformers import LlamaTokenizerFast# T5Tokenizer\n",
    "\n",
    "# 加载 SentencePiece model\n",
    "tokenizer = LlamaTokenizerFast(\n",
    "    vocab_file=\"lua-bpe-32k/lua-bpe-32k.model\",\n",
    "    unk_token=\"<|TOKEN:unk|>\",\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|im_end|>\",\n",
    "    pad_token=\"<|TOKEN:pad|>\",\n",
    ")\n",
    "\n",
    "# 保存为 Huggingface 规范目录，会自动生成 tokenizer.json\n",
    "tokenizer.save_pretrained(\"lua-bpe-32k-hf\")\n",
    "\n",
    "# 然后加载 fast 版本，并自动读取 tokenizer.json\n",
    "from transformers import AutoTokenizer\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"lua-bpe-32k-hf\")\n",
    "fast_tokenizer.save_pretrained(\"lua-bpe-32k-hf-V2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1f442-d22e-4d6f-86ee-2ff531169959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37008\n",
      "DEMO: ['▁function', '▁A', '=', '▁', '<|NULL|>']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_SET = set()\n",
    "class OpMode:\n",
    "    iNone = -1\n",
    "    iABC = 0\n",
    "    iABx = 1\n",
    "    iAsBx = 2\n",
    "    iAx = 3\n",
    "    @staticmethod\n",
    "    def tostr(mode):\n",
    "        if mode == OpMode.iABC:\n",
    "            return \"iABC\"\n",
    "        elif mode == OpMode.iABx:\n",
    "            return \"iABx\"\n",
    "        elif mode == OpMode.iAsBx:\n",
    "            return \"iAsBx\"\n",
    "        elif mode == OpMode.iAx:\n",
    "            return \"iAx\"\n",
    "        else:\n",
    "            return \"iNone\"\n",
    "class OpArgMask:\n",
    "    OpArgNone = -1\n",
    "    OpArgN = 0\n",
    "    OpArgU = 1\n",
    "    OpArgR = 2\n",
    "    OpArgK = 3\n",
    "    @staticmethod\n",
    "    def tostr(mask):\n",
    "        if mask == OpArgMask.OpArgN:\n",
    "            return \"OpArgN\"\n",
    "        elif mask == OpArgMask.OpArgU:\n",
    "            return \"OpArgU\"\n",
    "        elif mask == OpArgMask.OpArgR:\n",
    "            return \"OpArgR\"\n",
    "        elif mask == OpArgMask.OpArgK:\n",
    "            return \"OpArgK\"\n",
    "        else:\n",
    "            return \"OpArgNone\"\n",
    "OpDefines = [\n",
    "    # op-code | op-name | T | A | B | C | mode | inline | jump\n",
    "    (0, \"OP_MOVE\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (1, \"OP_LOADK\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgN, OpMode.iABx,\n",
    "     lambda i: False, False),\n",
    "    (2, \"OP_LOADKX\", 0, 1, OpArgMask.OpArgN, OpArgMask.OpArgN, OpMode.iABx,\n",
    "     lambda i: True, False),\n",
    "    (3, \"OP_LOADBOOL\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: i.C != 0, False),\n",
    "    (4, \"OP_LOADNIL\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (5, \"OP_GETUPVAL\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (6, \"OP_GETTABUP\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (7, \"OP_GETTABLE\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (8, \"OP_SETTABUP\", 0, 0, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (9, \"OP_SETUPVAL\", 0, 0, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (10, \"OP_SETTABLE\", 0, 0, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (11, \"OP_NEWTABLE\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (12, \"OP_SELF\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (13, \"OP_ADD\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (14, \"OP_SUB\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (15, \"OP_MUL\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (16, \"OP_MOD\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (17, \"OP_POW\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (18, \"OP_DIV\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (19, \"OP_IDIV\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (20, \"OP_BAND\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (21, \"OP_BOR\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (22, \"OP_BXOR\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (23, \"OP_SHL\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (24, \"OP_SHR\", 0, 1, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (25, \"OP_UNM\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (26, \"OP_BNOT\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (27, \"OP_NOT\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (28, \"OP_LEN\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (29, \"OP_CONCAT\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgR, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (30, \"OP_JMP\", 0, 0, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iAsBx,\n",
    "     lambda i: False, True),\n",
    "    (31, \"OP_EQ\", 1, 0, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (32, \"OP_LT\", 1, 0, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (33, \"OP_LE\", 1, 0, OpArgMask.OpArgK, OpArgMask.OpArgK, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (34, \"OP_TEST\", 1, 0, OpArgMask.OpArgN, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (35, \"OP_TESTSET\", 1, 1, OpArgMask.OpArgR, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (36, \"OP_CALL\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (37, \"OP_TAILCALL\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (38, \"OP_RETURN\", 0, 0, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (39, \"OP_FORLOOP\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iAsBx,\n",
    "     lambda i: False, True),\n",
    "    (40, \"OP_FORPREP\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iAsBx,\n",
    "     lambda i: False, True),\n",
    "    (41, \"OP_TFORCALL\", 0, 0, OpArgMask.OpArgN, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: True, False),\n",
    "    (42, \"OP_TFORLOOP\", 0, 1, OpArgMask.OpArgR, OpArgMask.OpArgN, OpMode.iAsBx,\n",
    "     lambda i: False, True),\n",
    "    (43, \"OP_SETLIST\", 0, 0, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iABC,\n",
    "     lambda i: i.C == 0, False),\n",
    "    (44, \"OP_CLOSURE\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABx,\n",
    "     lambda i: False, False),\n",
    "    (45, \"OP_VARARG\", 0, 1, OpArgMask.OpArgU, OpArgMask.OpArgN, OpMode.iABC,\n",
    "     lambda i: False, False),\n",
    "    (46, \"OP_EXTRAARG\", 0, 0, OpArgMask.OpArgU, OpArgMask.OpArgU, OpMode.iAx,\n",
    "     lambda i: False, False),\n",
    "    # 超长常规定义范围的指令\n",
    "    (-1, \"OP_UNKNOWN\", 0, 0, OpArgMask.OpArgNone, OpArgMask.OpArgNone, OpMode.iNone,\n",
    "     lambda i: False, False),\n",
    "]\n",
    "for opdef in OpDefines:\n",
    "    TOKEN_SET.add(f\"<|{opdef[1]}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-T={opdef[2]}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-A={opdef[3]}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-B={OpArgMask.tostr(opdef[4])}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-C={OpArgMask.tostr(opdef[5])}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-MODE={OpMode.tostr(opdef[6])}|>\")\n",
    "TOKEN_SET.add(\"<|Instruction|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-B-ISK=true|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-B-ISK=false|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-C-ISK=true|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-C-ISK=false|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-INLINE=true|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-INLINE=false|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-JUMP=true|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-JUMP=false|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-Bx|>\")\n",
    "TOKEN_SET.add(\"<|/Instruction-Bx|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-sBx|>\")\n",
    "TOKEN_SET.add(\"<|/Instruction-sBx|>\")\n",
    "TOKEN_SET.add(\"<|Instruction-Ax|>\")\n",
    "TOKEN_SET.add(\"<|/Instruction-Ax|>\")\n",
    "TOKEN_SET.add(\"<|/Instruction|>\")\n",
    "TOKEN_SET.add(\"<|Constant|>\")\n",
    "# 存在：nil|true|false\n",
    "TOKEN_SET.add(\"<|/Constant|>\")\n",
    "TOKEN_SET.add(\"<|Upvaldesc|>\")\n",
    "TOKEN_SET.add(\"<|/Upvaldesc|>\")\n",
    "TOKEN_SET.add(\"<|Upvaldesc-name|>\")\n",
    "TOKEN_SET.add(\"<|/Upvaldesc-name|>\")\n",
    "TOKEN_SET.add(\"<|NULL|>\")\n",
    "TOKEN_SET.add(\"<|LocVar|>\")\n",
    "TOKEN_SET.add(\"<|/LocVar|>\")\n",
    "TOKEN_SET.add(\"<|LocVar-varname|>\")\n",
    "TOKEN_SET.add(\"<|/LocVar-varname|>\")\n",
    "TOKEN_SET.add(\"<|LocVar-startpc|>\")\n",
    "TOKEN_SET.add(\"<|/LocVar-startpc|>\")\n",
    "TOKEN_SET.add(\"<|LocVar-endpc|>\")\n",
    "TOKEN_SET.add(\"<|/LocVar-endpc|>\")\n",
    "TOKEN_SET.add(\"<|LineInfo|>\")\n",
    "TOKEN_SET.add(\"<|LineInfo-pad|>\")\n",
    "TOKEN_SET.add(\"<|/LineInfo|>\")\n",
    "TOKEN_SET.add(\"<|Proto|>\")\n",
    "TOKEN_SET.add(\"<|/Proto|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizeupvalues|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizeupvalues|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizek|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizek|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizecode|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizecode|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizelineinfo|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizelineinfo|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizep|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizep|>\")\n",
    "TOKEN_SET.add(\"<|Proto-sizelocvars|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-sizelocvars|>\")\n",
    "TOKEN_SET.add(\"<|Proto-linedefined|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-linedefined|>\")\n",
    "TOKEN_SET.add(\"<|Proto-lastlinedefined|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-lastlinedefined|>\")\n",
    "TOKEN_SET.add(\"<|Proto-k|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-k|>\")\n",
    "TOKEN_SET.add(\"<|Proto-k-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-k-idx|>\")\n",
    "TOKEN_SET.add(\"<|Proto-k-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-k-idx|>\")\n",
    "TOKEN_SET.add(\"<|Proto-code|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-code|>\")\n",
    "TOKEN_SET.add(\"<|Proto-code-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-code-idx|>\")\n",
    "TOKEN_SET.add(\"<|Jump-Target|>\")\n",
    "TOKEN_SET.add(\"<|/Jump-Target|>\")\n",
    "TOKEN_SET.add(\"<|Proto-lineinfo|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-lineinfo|>\")\n",
    "TOKEN_SET.add(\"<|Proto-locvars|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-locvars|>\")\n",
    "TOKEN_SET.add(\"<|Proto-locvars-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-locvars-idx|>\")\n",
    "TOKEN_SET.add(\"<|Proto-upvalues|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-upvalues|>\")\n",
    "TOKEN_SET.add(\"<|Proto-upvalues-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-upvalues-idx|>\")\n",
    "TOKEN_SET.add(\"<|Proto-source|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-source|>\")\n",
    "TOKEN_SET.add(\"<|Proto-p|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-p|>\")\n",
    "TOKEN_SET.add(\"<|Proto-p-idx|>\")\n",
    "TOKEN_SET.add(\"<|/Proto-p-idx|>\")\n",
    "for i in range(0, 256):\n",
    "    TOKEN_SET.add(f\"<|Instruction-B-K={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-B-R={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-C-K={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-C-R={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-A={i}|>\")\n",
    "    TOKEN_SET.add(f\"\\\\x{i:02X}\")\n",
    "    TOKEN_SET.add(f\"<|Upvaldesc-instack={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Upvaldesc-idx={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Proto-nupvalues={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Proto-numparams={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Proto-is_vararg={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Proto-maxstacksize={i}|>\")\n",
    "for i in range(0, 512):\n",
    "    TOKEN_SET.add(f\"<|Instruction-B={i}|>\")\n",
    "    TOKEN_SET.add(f\"<|Instruction-C={i}|>\")\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lua-bpe-32k-hf-V2\")\n",
    "with open(\"Atokens.txt\", \"w\") as f:\n",
    "    for token in sorted(TOKEN_SET):\n",
    "        f.write(token + \"\\n\")\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": list(sorted(TOKEN_SET))+[\"<|im_start|>\",\"user\\n\",\"assistant\\n\"]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.save_pretrained(\"lua-bpe-32k-Add\")\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lua-bpe-32k-Add\")\n",
    "print(len(tokenizer))\n",
    "print(\"DEMO:\", tokenizer.tokenize(\"function A= <|NULL|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9416316-2a02-4b89-91a4-644203d04274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: lua-bpe-32k-Add/ (stored 0%)\n",
      "  adding: lua-bpe-32k-Add/tokenizer_config.json (deflated 95%)\n",
      "  adding: lua-bpe-32k-Add/special_tokens_map.json (deflated 98%)\n",
      "  adding: lua-bpe-32k-Add/added_tokens.json (deflated 85%)\n",
      "  adding: lua-bpe-32k-Add/tokenizer.model (deflated 49%)\n",
      "  adding: lua-bpe-32k-Add/tokenizer.json (deflated 87%)\n",
      "  adding: lua-bpe-32k-Add/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: lua-bpe-32k-Add/.ipynb_checkpoints/added_tokens-checkpoint.json (deflated 85%)\n",
      "  adding: lua-bpe-32k-Add/.ipynb_checkpoints/special_tokens_map-checkpoint.json (deflated 98%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r lua-bpe-32k-Add.zip lua-bpe-32k-Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19791e00-ae5c-4a6a-bf3b-c82e0225ec25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
